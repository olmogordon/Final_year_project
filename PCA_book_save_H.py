#!/usr/bin/env python
# coding: utf-8

# In[66]:


import numpy as np
import math
import random
import matplotlib.pyplot as plt
import string
from collections import Counter
from scipy import linalg
from sklearn.decomposition import PCA
import matplotlib.colors as mcolors
import pickle


# In[2]:


# squared error calculation function
def error(x, y, w_1, w_2):
    y_hat = np.matmul(w_2, np.matmul(w_1, x))
    return np.sum((y - y_hat)**2)

# weight matrix gradient calculation function
def gradient(x, y, w_1, w_2, n_1, n_2, n_3):

    #linear function of the deep neural network
    h = np.matmul(w_1, x)
    y_hat = np.matmul(w_2, h)

    x_matrix = np.reshape(x, (x.shape[0],1))
    h_matrix = np.reshape(h, (h.shape[0],1))
    y_difference = np.reshape((y - y_hat), (y.shape[0], 1))
    
    delta_w_1 = np.matmul(np.transpose(w_2) ,np.matmul(y_difference, np.transpose(x_matrix)))
    delta_w_2 = np.matmul(y_difference, np.transpose(h_matrix))
    
    return delta_w_1, delta_w_2

# clean text to be processed keeping full stops
def txt_full_stops(filename):
    file = open(filename, 'rt')
    text = file.read()
    text = text.split()
    text_lower = [w.lower() for w in text] 
    text_clean = [w.replace(',', '') for w in text_lower]
    text_final = [w for w in text_clean if w!='']
    
    return text_final

# function that returns N most frequent words from a given txt file and it returns a library of these words indexed in order.
def word_counter(filename, N):
    #load text
    file = open(filename, 'rt')
    text = file.read()
    
    # split into words by white space
    words = text.split()
    
    # clean up the words that appear in the book
    table = str.maketrans('', '', string.punctuation)
    stripped = [w.translate(table) for w in words] #remove punctuation
    #stripped1 = [w.strip('') for w in stripped]
    word_clean = [w.lower() for w in stripped] #convert to lower case
    words_clean = [ w for w in word_clean if w!='']
    
    #get the N most frequent words. Returns a list with ('word', frequency) entries
    text_words = Counter(words_clean)
    most_occur = text_words.most_common(N)

    #transform into a list with just the N most frequnt words
    words_list = []
    word_to_index = {}
    for i in range(N-1):
        words_list.append(most_occur[i][0])
        word_to_index[words_list[i]]=i
    
    words_list.append('OTHER')
    word_to_index['OTHER'] = N-1
    
    return words_list, word_to_index

# encoding function from words to one-hot vectors taking the (word, index) library generated by word_counter()
def word_to_onehot(word, word_to_index):
    n = len(word_to_index)
    one_hot=np.zeros(n)
    if(word in word_to_index):
        one_hot[word_to_index[word]]=1
    else:
        one_hot[n-1]=1

    return one_hot


# In[59]:


# function that does the learning and stores hidden layer activation values to perform PCA afterwards
def H(X, n_1, n_2, n_3, lamda, lower, upper, w_1, w_2, trial_n, text_final, index_map, freq_words, freq):
    #h matrix with final values of middle layer activations
    h_final = np.zeros((n_2, n_1))
    
    #matrix to store all hidden layer activations along the way
    H = np.zeros((trial_n * (len(text_final)//freq),n_2,n_1))
    
    #counter for updating entry of H matrix
    t = 0
    
    #learning of network over trial_n trials
    for trial in range(trial_n):
        x=word_to_onehot(text_final[0], index_map)
        for i in range(1,len(text_final)):
            if (text_final[i] in freq_words):
                y=word_to_onehot(text_final[i], index_map)
                
                dw_1, dw_2 = gradient(x, y, w_1, w_2, n_1, n_2, n_3)
                w_1 = w_1 + lamda * dw_1
                w_2 = w_2 + lamda * dw_2
                if (i % freq == 0):
                    H[t,:,:] = np.matmul(w_1,X)
                    t += 1
                x = y
            else:
                i+=1
                if i<len(text_final):
                    while not(text_final[i] in freq_words):
                        i+=1
                    x=word_to_onehot(text_final[i], index_map)

    #final activation h values after learning, should be "n_2" "n_1" dimensional vectors.
    h_final = np.matmul(w_1,X)
    H[-1] = h_final
    
    return H


# In[68]:


#clean text keeping full stops
text_final = txt_full_stops('metamorphosis_clean.txt')

#count most frequent words and output a list of them together with an indexed dictionary
metamorphosis_words, met_index_map = word_counter('metamorphosis_clean.txt', 1000)

# change most frequent words to one-hot vectors one by one in order
one_hot_met = []
for word in metamorphosis_words:
    one_hot_met.append(word_to_onehot(word, met_index_map))
    
# convert to a np.array to make future operations easier (matmul, loop through it, etc)
# this is input matrix X of all the inputs to calculate hidden layer activation matrix at each stage
X = np.asarray(one_hot_met)


# In[60]:


n_1 = 1000
n_2 = 500
n_3 = 1000
lower = -0.001
upper = 0.001
lamda = 0.1
freq = 300
trial_n = 1


# In[61]:


w_1 = np.random.uniform(lower,upper,[n_2, n_1])
w_2 = np.random.uniform(lower,upper,[n_3, n_2])


# In[63]:


H = H(X, n_1, n_2, n_3, lamda, lower, upper, w_1, w_2, trial_n, text_final, met_index_map, metamorphosis_words, freq)


# In[67]:


pickle.dump(H, open("H_1trial_500h_300freq.pickle", "wb"))


# In[ ]:


#==========================================================================================


# In[69]:


n_1 = 1000
n_2 = 500
n_3 = 1000
lower = -0.001
upper = 0.001
lamda = 0.1
freq = 800
trial_n = 10


# In[70]:


w_1 = np.random.uniform(lower,upper,[n_2, n_1])
w_2 = np.random.uniform(lower,upper,[n_3, n_2])


# In[ ]:


H_10_trials = H(X, n_1, n_2, n_3, lamda, lower, upper, w_1, w_2, trial_n, text_final, met_index_map, metamorphosis_words, freq)


# In[ ]:


pickle.dump(H_10_trials, open("H_10trials_500h_800freq.pickle", "wb"))


# In[ ]:


#===============================================================================================


# In[ ]:


n_1 = 1000
n_2 = 500
n_3 = 1000
lower = -0.001
upper = 0.001
lamda = 0.1
freq = 1000
trial_n = 20


# In[ ]:


w_1 = np.random.uniform(lower,upper,[n_2, n_1])
w_2 = np.random.uniform(lower,upper,[n_3, n_2])


# In[ ]:


H_20_trials = H(X, n_1, n_2, n_3, lamda, lower, upper, w_1, w_2, trial_n, text_final, met_index_map, metamorphosis_words, freq)


# In[ ]:


pickle.dump(H_20_trials, open("H_20trials_500h_800freq.pickle", "wb"))


# In[ ]:




